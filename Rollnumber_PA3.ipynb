{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3 - Word Embeddings\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will learn how to train your own word embeddings using two approaches, then explore some of the fun things you can do with them.\n",
    "\n",
    "Word Embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "For reference and additional details, please go through Chapter 6 of [the SLP3 book](https://web.stanford.edu/~jurafsky/slp3) and this [nice writeup by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Implementing `word2vec` [80 points]\n",
    "\n",
    "In this part, you will implement the `word2vec` algorithm. \n",
    "\n",
    "While `word2vec` is more of a _framework_ for learning word embeddings, we will focus on the `SkipGram` model, specifically how it was trained in the original 2013 paper. Your primary references for understanding this algorithm will be the lecture slides, and the two aforementioned links.\n",
    "\n",
    "You will be working with the *greatest* work of literature ever written: ~~the Bee Movie script~~ **The Lord of the Rings** - specifically the first book, *The Fellowship of the Ring*. \n",
    "\n",
    "While this book is a masterpiece, it can take a while to train embeddings on the entire text. So, we will be working with a subset of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in the libraries\n",
    "# Note: you are NOT allowed to use any other libraries or functions outside of these\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our raw data\n",
    "\n",
    "In the cell below, you will read in the data as one very long string.\n",
    "\n",
    "This will be followed by creating a `Dataset` class that will be helpful in working with your dataset when training the model. The `Dataset` class should have the following attributes/methods:\n",
    "\n",
    "- `__init__(self, data)` - the constructor that takes in the data and initializes the necessary attributes.\n",
    "\n",
    "- `data` - the data that is passed in. You can apply a very simple preprocessing pipeline: (1) substitute all punctuation with a period (i.e. `.`), (2) lowercase all the text, and (3) extract only those characters that are alphabetic or a period.\n",
    "\n",
    "- `tokens` - a list of all the tokens in the data. It might be helpful to use the `nltk.word_tokenize` function already imported for you.\n",
    "\n",
    "- `vocab` - a set of all the unique tokens in the data. Be sure to sort this and convert it to a list as to have a consistent ordering.\n",
    "\n",
    "- `vocab_size` - the length of the vocabulary.\n",
    "\n",
    "- `stoi` - a mapping from the word (s) to their index (i) in the vocab. It is important to have sorted your vocab before creating this mapping.\n",
    "\n",
    "- `itos` - a mapping from the index (i) to the word (s) in the vocab.\n",
    "\n",
    "The two mappings will be helpful in fetching your Embeddings later on, since your Embeddings will be a matrix of shape `(vocab_size, embedding_dim)` and the ordering will be dependent on your vocabulary's ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "with open(\"./The Fellowship of the Ring.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[:200]) # print out the first 200 chars\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, data: str):\n",
    "        \n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "dataset = Dataset(data)\n",
    "print(f\"Number of tokens in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset\n",
    "\n",
    "Now for the fun part of the dataset preparation: creating the windows!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://jalammar.github.io/images/word2vec/skipgram-sliding-window-5.png\">\n",
    "</center>\n",
    "\n",
    "Recall in class you learned about sliding a window over the text to create the `(context, target)` pairs. You will implement this in the function below. \n",
    "\n",
    "We will adopt the following convention: the target word is at the center of a window, and the context words are the words surrounding the target word, with `ctx_size` tokens on either side.\n",
    "\n",
    "You will implement this to work with a list of tokens (whether that be in string-form, or as indices). The function should return a list of tuples, where each tuple is a pair of the form `(context, target)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The `range` function will return you an iterator where you can specify the start and final indices, as well as the jumps in between. Use this in constructing your for loop.</li>\n",
    "    <li>One easy way to do this is to begin the loop at the position corresponding to the first center word, then to grab the words to the left and the right in one list. After doing this, your loop moves to the next iter. Be careful with the starting and end iteration variables for your loop.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset using the sliding window approach\n",
    "def get_windows(\n",
    "        data: List[Union[str, int]], \n",
    "        ctx_size: int\n",
    "        ):\n",
    "    '''\n",
    "    Generates the windows to be used later for dataset creation\n",
    "\n",
    "    Takes in a list of tokens (as strings or integers/indices) and a context size\n",
    "    This will slide a window of size 2*ctx_size + 1 over the data producing\n",
    "    the a list of center words, and the corresponding context words\n",
    "\n",
    "    Note that ctx_size is the number of context words on EITHER side of the center word\n",
    "    e.g.\n",
    "    > get_windows([\"I\", \"love\", \"my\", \"dog\", \"!\"], 2) produces the first (and only) window to be\n",
    "    ([\"I\", \"love\", \"dog\", \"!\"], \"my\")\n",
    "    '''\n",
    "    \n",
    "    outside_words = []\n",
    "    center_words = []\n",
    "\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "\n",
    "    return outside_words, center_words\n",
    "\n",
    "owords, cwords = get_windows(\n",
    "    word_tokenize(\"hello how are you. i am under the water. please help me.\"),\n",
    "    2\n",
    ")\n",
    "\n",
    "for i in range(len(owords)):\n",
    "    print(f\"{owords[i]} ---> {cwords[i]}\")\n",
    "\n",
    "assert owords[1] == ['how', 'are', '.', 'i'] and cwords[1] == 'you' and len(owords) == 11, \\\n",
    "    \"Test failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to creating our model, recall that a key component of the algorithm was **Negative Sampling** so that our model was able to see occurences of words that _didn't_ appear in the context. We will implement this in the next part.\n",
    "\n",
    "Your `sample_neg_word` is a utility function that will continue sampling a word from the specific vocabulary until it is not the word you have specified. \n",
    "\n",
    "When actually implementing the negative sampling, you will sample tokens that are _not_ the center word only - many implementations ignore the exclusion of the context words, so we will do this same simplification here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use the `np.random.choice` function to sample an element from a list.</li>\n",
    "    <li>You can think about running a loop where you keep sampling until you have something that isn't the word to exclude. This is the only check you must make.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the dataset, we need to get positive and negative samples according to the windows made\n",
    "\n",
    "def sample_neg_word(to_exclude: str,\n",
    "                    vocab: List[str]):\n",
    "        '''\n",
    "        Samples a negative word from the vocab, excluding the word to_exclude\n",
    "        '''\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "\n",
    "        return sample_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the actual model\n",
    "\n",
    "Now you will make use of the `Dataset` class and the `sample_neg_word` function to implement the `SkipGram` model.\n",
    "\n",
    "Recall the steps of the model:\n",
    "\n",
    "1. Randomly initialize two matrices: `W` and `C` of shape `(vocab_size, embedding_dim)`. These will be your center/target word and context embeddings respectively.\n",
    "\n",
    "2. Being looping through each `(context, target)` pair in your dataset.\n",
    "\n",
    "    2.1. For each pair, sample `K` negative words from the vocabulary.\n",
    "\n",
    "    2.2. Compute the loss for the context and target word, as well as the negative samples.\n",
    "\n",
    "    2.3. Compute the gradients for the context and target word, as well as the `K` negative samples.\n",
    "\n",
    "    2.4. For each of these computed gradients, update the corresponding embeddings.\n",
    "\n",
    "3. Repeat this process for `num_epochs`.\n",
    "\n",
    "\n",
    "Recall the formulas for the Loss function and the gradients:\n",
    "\n",
    "$$L_{CE} = -\\log \\sigma(c_{pos} \\cdot w) - \\sum_{i=1}^{K} \\log \\sigma(-c_{neg_i} \\cdot w)$$\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial w} = [\\sigma(c_{pos} \\cdot w) - 1]c_{pos} + \\sum_{i=1}^{K} [\\sigma(c_{neg_i} \\cdot w)]c_{neg_i}$$\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial c_{pos}} = [\\sigma(c_{pos} \\cdot w) - 1]w$$\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial c_{neg_i}} = \\sigma(c_{neg_i} \\cdot w)w$$\n",
    "\n",
    "Where $c_{pos}$ is the context word, $w$ is the target word, and $c_{neg_i}$ is the $i^{th}$ negative sample.\n",
    "\n",
    "#### Implementation notes\n",
    "\n",
    "- You will be implementing all of this in the `word2vec` class, inside the `fit` function. Your embedding matrices will be initialized in the `__init__` function, according to the arguments passed in.\n",
    "\n",
    "- You **must** print out your loss after every epoch (i.e. after every pass through the dataset). If you have implemented everything perfectly, you should see the loss decreasing over time, with no numerical overflows/underflows or the like.\n",
    "\n",
    "- Your `fit` function will return the list of losses over each epoch - this will be used to visualize the training process.\n",
    "\n",
    "- <font size=\"3\" color=\"red\"><b>Do not tamper with code you haven't been permitted to modify.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "# The dataset is very large, so we will only use the first 2000 tokens for now\n",
    "# There will be 3 words on each side of the center word\n",
    "context_words, target_words = get_windows(\n",
    "    dataset.tokens[:2000], \n",
    "    ctx_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>A simple (but slightly inefficient) way to implement this would be to use two `for` loops (ignoring the one for sampling the negative words): one for the center word/window, an inner one for each context word so you have a (ctx, target) pair. This is done since you are passing the windows as they are into the function.</li>\n",
    "    <li>Be VERY careful about which variable is a string, and which is the corresponding index in the vocab. You can use the `encode` function to move from string to index.</li>\n",
    "    <li>Print out the shapes or use `assert` statements to ensure the shapes are the way you'd expect them. Broadcasting can mess things up if you're not careful.</li>\n",
    "    <li>Never forget: the gradients that are being calculated will ONLY update the embeddings for the corresponding tokens in the matrices, NOT THE ENTIRE MATRIX.</li>\n",
    "    <li>An easy way to do this is to make a matrix of zeros representing the gradients for one of the Embedding matrices. When you create the gradient vector(s) for the target/context/negative samples, you can simply replace the corresponding row in that zeros matrix with that vector. The update equation will be very easy to implement.</li>\n",
    "    <li>If you're running into overflows/underflows or numerical instability, there is something wrong with your implementation. If you're sure everything is correct, double-check your equations with the lecture slides - one wrong sign for a gradient cost me an hour of debugging ;-;</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x : np.ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class word2vec:\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size : int, \n",
    "                 stoi : dict, \n",
    "                 emb_dim : int):\n",
    "\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "\n",
    "    def __call__(self, x : int):\n",
    "        return (self.W[x] + self.C[x]) / 2\n",
    "    \n",
    "    \n",
    "    def encode(self, x : Union[str, List[str]]):\n",
    "        return [dataset.stoi[i] for i in x] if isinstance(x, list) else self.stoi[x]\n",
    "\n",
    "    def fit(self, \n",
    "            context_words : List[List[str]], \n",
    "            target_words : List[str], \n",
    "            num_epochs : int = 5, \n",
    "            learning_rate : float = 0.001, \n",
    "            K : int = 5):\n",
    "        '''\n",
    "        Runs the training algorithm for the word2vec model\n",
    "        '''\n",
    " \n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            ## Your code here\n",
    "            \n",
    "            ## --             \n",
    "\n",
    "            epoch_loss = epoch_loss / len(target_words)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.3f}\")\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "        return losses\n",
    "\n",
    "w2v_model = word2vec(vocab_size=dataset.vocab_size,\n",
    "                     stoi=dataset.stoi,\n",
    "                     emb_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = w2v_model.fit(context_words,\n",
    "                       target_words,\n",
    "                       num_epochs=10)\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curve for word2vec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Hopefully you were able to implement and train the model successfully!\n",
    "\n",
    "Now to actually examine the embeddings you've trained. We will do one small exercise involving checking the similarity of pairs of words in the vocabulary.\n",
    "\n",
    "This means you will need to implement the Cosine Similarity function. Recall that the Cosine Similarity between two vectors $u$ and $v$ is given by:\n",
    "$$ \\text{Cosine Similarity}(u, v) = \\frac{u \\cdot v}{||u|| \\cdot ||v||} $$\n",
    "\n",
    "You will implement this in the `cosine_similarity` function below. You will then use this function on a series of predefined word pairs to see how similar they are in the embedding space: this entails that they showed up in similar contexts in the text.\n",
    "\n",
    "Since most people are not cultured enough to have read the Lord of the Rings, here are some notes to make sense of what you _should_ see:\n",
    "\n",
    "- `Frodo` and `Sam` are best friends, so they should have a high similarity.\n",
    "\n",
    "- `Gandalf` has been referred to as Gandalf the `White`.\n",
    "\n",
    "- `Frodo` must make an arduous journey to `Mordor`.\n",
    "\n",
    "- `Aragorn` is close to Frodo, but hasn't been to his home of the `Shire`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    '''\n",
    "    Implements the cosine similarity for vectors u and v\n",
    "    '''\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "    return sim\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"frodo\"]],\n",
    "    w2v_model.C[dataset.stoi[\"sam\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"gandalf\"]],\n",
    "    w2v_model.C[dataset.stoi[\"white\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"mordor\"]],\n",
    "    w2v_model.C[dataset.stoi[\"frodo\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    w2v_model.W[dataset.stoi[\"shire\"]],\n",
    "    w2v_model.C[dataset.stoi[\"aragorn\"]]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learning Embeddings with Neural Networks [50 points]\n",
    "\n",
    "Now to spice things up.\n",
    "\n",
    "In this part, you will frame the learning problem as a Neural Network trying to predict the center word of a window, given some _pooled_ representation of the context words. This is an updated version of the `word2vec` algorithm, only coming out the year after the original 2013 paper.\n",
    "\n",
    "You will create a Neural Network to do this task, with the following setup:\n",
    "\n",
    "- The model will take in a matrix of size `(batch_size, vocab_size)` representing the context words' pooled representation. We will define how this pooling is done later.\n",
    "\n",
    "- The model will output a matrix of size `(batch_size, vocab_size)` representing the probability distribution over the vocabulary.\n",
    "\n",
    "- The model will have one hidden layer of size `emb_dim`. This means that there are two weight matrices (and bias vectors) to be learned - these will form the Embeddings for our tokens.\n",
    "\n",
    "- The same dataset as before will be used, so we will use the `get_windows` function in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in the libraries\n",
    "# Note: you are NOT allowed to use any other libraries or functions outside of these\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the dataset again\n",
    "context_words, target_words = get_windows(\n",
    "    dataset.tokens[:2000], \n",
    "    ctx_size=3\n",
    ")\n",
    "\n",
    "# Encode the entirety of the context words\n",
    "encode = lambda x: [dataset.stoi[c] for c in x] if isinstance(x, list) else dataset.stoi[x]\n",
    "context_words = [encode(cw) for cw in context_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Dataset\n",
    "\n",
    "Again, the input will be a pooled representation of the context words. When we batch multiple windows together, the input matrix will be of size `(batch_size, vocab_size)`.\n",
    "\n",
    "First, we will represent each individual _word_ as a One-Hot Vector (since we're learning Embeddings, we have nothing better, in this situation). \n",
    "\n",
    "Now, for each window, suppose you have $C$ context words, and one target word. This means if you stack up all your context words' One-Hot representations, you will have a `(C, vocab_size)` matrix.\n",
    "\n",
    "We can pool these by simply taking the mean along the first axis, such that the result would be a single vector of length `vocab_size`.\n",
    "\n",
    "---\n",
    "\n",
    "This operation would look like the following (suppose the `vocab_size` is 5 for this example, and we have 4 context words):\n",
    "\n",
    "1. Stack up the One-Hot Vectors:\n",
    "```python\n",
    "    [\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0]\n",
    "    ]\n",
    "```\n",
    "\n",
    "2. Take the mean along the first axis to get your pooled representation for a single window:\n",
    "```python\n",
    "    [0.25, 0.25, 0  , 0.5 , 0]\n",
    "```\n",
    "\n",
    "3. The result of step 2 will be an input to the Neural Network. You can batch together multiple of these windows to get a matrix of `(batch_size, vocab_size)`.\n",
    "\n",
    "--- \n",
    "\n",
    "In the cell below, you will define\n",
    "\n",
    "- A function to `one_hot_encode` a single vector. This is as simple as filling in a vector of `vocab_size` zeros with a one at some index.\n",
    "\n",
    "- A function to do the following operation: `context_words_to_vector` will stack the input context words and take the mean in the fashion described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = context_words[0]\n",
    "y = encode(target_words[0])\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "\n",
    "\n",
    "def context_words_to_vector(context, vocab_size):\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "\n",
    "X = np.array([context_words_to_vector(cw, vocab_size) for cw in context_words])\n",
    "\n",
    "# The targets will be the indices of the target words, so this is just a vector\n",
    "y = np.array([encode(t) for t in target_words])\n",
    "\n",
    "# You can verify the correctness by looking at the shapes (or by running the test example in the markdown above)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Neural Language Model\n",
    "\n",
    "Now, you will create your model to do the objective mentioned before.\n",
    "\n",
    "Since this is a Multiclass Classification objective, i.e. the model will be predicting the probability distribution of the target word over the vocab, you will have to use the Softmax function.\n",
    "\n",
    "Your objective here is to complete the implementation of the `NeuralLM` class defined below.\n",
    "\n",
    "Note that since we have a single hidden layer (which should use the `tanh` activation function), the first weight matrix will be of shape `(vocab_size, emb_dim)` and the second will be of shape `(emb_dim, vocab_size)`. We will use these two weight matrices to represent our Embedding matrix, since all we need is a vector of size `emb_dim` for each token in the vocab :)\n",
    "\n",
    "Your forward pass will look like the following:\n",
    "\n",
    "$$Z = X W^{[1]} + b^{[1]}$$\n",
    "$$A = \\tanh(Z)$$\n",
    "$$\\hat Y = A W^{[2]} + b^{[2]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The derivative of tanh(z) is 1 - tanh(z)**2</li>\n",
    "    <li>You might find it helpful to reference the tutorial notebook for Neural Networks on LMS.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=-1, keepdims=True)\n",
    "\n",
    "class NeuralLM:\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.W1 = np.random.randn(vocab_size, emb_dim)\n",
    "        self.b1 = np.zeros(emb_dim)\n",
    "        self.W2 = np.random.randn(emb_dim, vocab_size)\n",
    "        self.b2 = np.zeros(vocab_size)\n",
    "\n",
    "        # Caching the activations and the gradients can make things very convenient\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Performs the forward pass for our Neural Net\n",
    "        '''\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "        return yhat\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        '''\n",
    "        Performs the backward pass for our Neural Net, computing the gradients required for updates\n",
    "        '''\n",
    "\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "\n",
    "    def update(self, lr):\n",
    "        '''\n",
    "        Updates the parameters, according to the Gradient Descent algorithm\n",
    "        '''\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "\n",
    "    def fit(self, x, y, epochs=10, lr=0.01):\n",
    "        '''\n",
    "        Runs the entire training loop for the specific number of epochs\n",
    "        '''\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            yhat = self.forward(x)\n",
    "            self.backward(x, y)\n",
    "            self.update(lr)\n",
    "            \n",
    "            # Compute loss\n",
    "            logprobas = np.log(softmax(yhat))\n",
    "            loss = -logprobas[range(len(yhat)), y].mean()\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model and getting the Embeddings\n",
    "\n",
    "Run the cell below to train your model, and plot the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size, emb_dim=100)\n",
    "model.fit(X, y, epochs=50, lr=0.01)\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curve for NeuralLM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've hopefully seen the model improving, we can extract its weight matrices to use as the Embeddings.\n",
    "\n",
    "We have the choice of using either one, or we could do something really funny and take the average of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings from the model\n",
    "embedding_matrix = (model.W1 + model.W2.T) / 2\n",
    "\n",
    "print(cosine_similarity(\n",
    "    embedding_matrix[dataset.stoi[\"frodo\"]],\n",
    "    embedding_matrix[dataset.stoi[\"sam\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    embedding_matrix[dataset.stoi[\"gandalf\"]],\n",
    "    embedding_matrix[dataset.stoi[\"white\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    embedding_matrix[dataset.stoi[\"mordor\"]],\n",
    "    embedding_matrix[dataset.stoi[\"frodo\"]]\n",
    "))\n",
    "\n",
    "print(cosine_similarity(\n",
    "    embedding_matrix[dataset.stoi[\"shire\"]],\n",
    "    embedding_matrix[dataset.stoi[\"aragorn\"]]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about whether these performed better than the previous ones, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Playing with Word Vectors [20 points]\n",
    "\n",
    "The intensive parts of this assignment are over - now we'll play with pretrained embeddings, i.e. embeddings that someone else has trained.\n",
    "\n",
    "We will use the GloVe embeddings from `gensim`, a Python library made for interacting with word vectors.\n",
    "\n",
    "In the cells below, we will make our imports, load in our embeddings, and construct our numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim.downloader as api\n",
    "from pprint import pprint\n",
    "\n",
    "def load_embedding_model():\n",
    "    '''\n",
    "    Loads the GloVe embeddings from gensim\n",
    "    '''\n",
    "    gensim_wv = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(f\"Loaded embeddings with vocab size {len(gensim_wv.key_to_index)} with vector size {gensim_wv.vector_size}\")\n",
    "    return gensim_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Embeddings (this can take ~8 minutes)\n",
    "gensim_wv = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(gensim_wv):\n",
    "    \n",
    "    # Get the words in the vocab\n",
    "    words = list(gensim_wv.index_to_key)\n",
    "    stoi = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    \n",
    "    # Add the words to the matrix M\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(gensim_wv.get_vector(w))\n",
    "            stoi[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # Convert the list of vectors to a numpy matrix\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    return M, stoi\n",
    "\n",
    "M, stoi = get_embedding_matrix(gensim_wv)\n",
    "print(f\"Shape of the embedding matrix: {M.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our embeddings\n",
    "\n",
    "Now that we've created our matrix, let's work on visualizing them.\n",
    "\n",
    "The issue with these embeddings is that they are in 200 dimensions. Most humans can't see beyond 3 dimensions, and it's convenient to plot in 2.\n",
    "\n",
    "One nifty trick we can do to _squish_ down a vector in higher dimensions, to something in fewer dimensions, is to utilize **Dimensionality Reduction** techniques. This will learn the ambient structure in the data, and use it to capture as much information (technically, the \"variance\") in the amount of dimensions you want.\n",
    "\n",
    "Most people go with [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) or [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) for this. We will go with a variant of [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition), a method to factorize a matrix.\n",
    "\n",
    "You can read up on the documentation for the `sklearn` implementation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
    "\n",
    "In the cell below, implement the `reduce_to_k_dim` algorithm, where you run `TruncatedSVD` to squish your `(vocab_size, emb_dim)` matrix to `(vocab_size, K)`, where `K` is much smaller than `emb_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    '''\n",
    "    Reduce a matrix of shape (num_words, num_dim) to (num_words, k) dimensions\n",
    "    '''\n",
    "    n_iters = 10\n",
    "    print(f\"Running Truncated SVD over {n_iters} iterations...\")\n",
    "\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "\n",
    "    return M_reduced\n",
    "\n",
    "# Reduce the matrix to 2 dimensions\n",
    "M_reduced = reduce_to_k_dim(M, k=2)\n",
    "\n",
    "# Normalize the rows to make them of unit length (helps with visualization)\n",
    "M_reduced_unit = M_reduced / np.linalg.norm(M_reduced, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Shape of the reduced matrix: {M_reduced_unit.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you can plot out the embeddings from the reduced matrix. Note that since we squished the information coming from 200 dimensions into just 2, we won't have a perfect visualization by any means, but it's still worth studying.\n",
    "\n",
    "In the cell below, you can fill `words_to_plot` with words whose embeddings you'd like to see in a scatterplot. If you wish to join pairs of words, you can pass them in as pairs in the `pairs_to_join` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced_unit, words, stoi, pairs_to_join):\n",
    "    '''\n",
    "    Produces a scatterplot of the embeddings with the words annotated\n",
    "\n",
    "    Parameters:\n",
    "    M_reduced_unit : np.ndarray\n",
    "        The reduced matrix of embeddings\n",
    "    words : List[str]\n",
    "        The words to annotate\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    for i, txt in enumerate(words):\n",
    "        word_ind = stoi.get(txt)\n",
    "        ax.scatter(M_reduced_unit[word_ind, 0], M_reduced_unit[word_ind, 1])\n",
    "        ax.annotate(txt, (M_reduced_unit[word_ind, 0], M_reduced_unit[word_ind, 1]))\n",
    "\n",
    "    for pair in pairs_to_join:\n",
    "        w1, w2 = pair\n",
    "        w1_ind = stoi.get(w1)\n",
    "        w2_ind = stoi.get(w2)\n",
    "        ax.plot([M_reduced_unit[w1_ind, 0], M_reduced_unit[w2_ind, 0]], \n",
    "                [M_reduced_unit[w1_ind, 1], M_reduced_unit[w2_ind, 1]], 'k-')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "words_to_plot = [\"berlin\", \"germany\", \"paris\", \"france\", \"rome\", \"italy\", \"london\", \"england\"]\n",
    "pairs_to_join = [(\"berlin\", \"germany\"), (\"paris\", \"france\"), (\"rome\", \"italy\"), (\"london\", \"england\")]\n",
    "plot_embeddings(M_reduced_unit, words_to_plot, stoi, pairs_to_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies with Word Vectors\n",
    "\n",
    "Recall from the lectures that word vectors let us capture relationships between words. This means we can use vector arithmetic to create _analogies_.\n",
    "\n",
    "For example, if we had an embedding matrix E, and we wanted to find the relationship between `king` and `man`, and `queen` and `woman`, we would find\n",
    "\n",
    "$$E[\\text{king}] - E[\\text{man}] \\approx E[\\text{queen}] - E[\\text{woman}]$$\n",
    "\n",
    "`gensim` makes this really easy for us. To save time, we can use the `most_similar` function to capture the nearest neighbors to the vector you get when \"constructing the parallelogram\" (from lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"king is to man, as ??? is to woman\"\n",
    "gensim_wv.most_similar(\n",
    "    positive=['woman', 'king'],\n",
    "    negative=['man']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not perfect by any means. Run the cell below to see one case of the arithmetic failing.\n",
    "\n",
    "Write a few words about why this might be the case - there's a very reasonable explanation, provided you don't use the metric system ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"glove is to hand as ??? is to foot\"\n",
    "gensim_wv.most_similar(\n",
    "    positive=['foot', 'glove'],\n",
    "    negative=['hand']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green\"> Write your answer here. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, it's important to know that biases and stigmas are implicit inside these word embeddings. \n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". \n",
    "\n",
    "Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint(gensim_wv.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
    "print('-'*25)\n",
    "pprint(gensim_wv.most_similar(positive=['woman', 'profession'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green\"> Write your answer here. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
